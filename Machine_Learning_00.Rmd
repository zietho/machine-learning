---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 1.0.5
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
# @hidden_cell
# %autosave 0
```

```{python}
import pandas as pd
import numpy as np
import os as os
import matplotlib.pyplot as plt

import re
import nltk
```

# ///////////////////////////////////////////////////////
Form for report:

### Explanation
Explanation of choice for data sets

### Characteristics

* How many samples
* How many attributes (features)
* What types of attributes (nominal, ordinal, interval, ...)
* Distribution/histograms of values in the attributes

# ///////////////////////////////////////////////////////


# Data Set 1: Twitter sentiment140

Abstract: This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment. <sup>1</sup>

Dataset Source <sup>1</sup> [twitter dataset](https://www.kaggle.com/kazanova/sentiment140) 

https://www.kaggle.com/kazanova/sentiment140

## Explanation
Since we were interested in doing text analysis in particular sentiment analysis and since this dataset has a very limited amount of features, while having lots of samples we dediced to take this data set. 

## Characteristics

### How many samples: 

Dataset has 1.600.000 tweets (samples) and 9 columns, one of them being the feature.

#### Short description of the dataset columns
* target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive) [dtype = int64]
* ids: The id of the tweet (2087) [dtype = int64]
* date: the date of the tweet (Sat May 16 23:58:44 UTC 2009) [dtype = datetime]
* flag: The query (lyx). If there is no query, then this value is NO_QUERY. [dtype = object]
* user: the user that tweeted (max) [dtype = object]
* text: the text of the tweet (this is a sample tweet) [dtype = object]

### How many attributes (features): 

The aim is to extract features by taking the tweet text from each datapoint and splitting the single whole text into words to classify them, weather they have positive or negative meaning. Using dictionaries that contain words which represent specific emotions we decide if a tweet is positive, negative or neutral. 
<br>

So far we cleaned the tweets by applying some regex to handle URL's, references to other twitter-users, additional whitespace, turning the text into lowercase and more to prepare for further analysis.
<br>

Based on the method, tool and dictionary that we will use in the future we will end up having a fairly large amount of columns (features) each representing the quantity of a word that is contained in each tweet. If the positive amount of words will dominate then the tweet will be labeled with a positive sentiment.


### What types of attributes (nominal, ordinal, interval, ...):

* target: nominal 
* ids: nominal
* date: nominal
* flag: nominal
* user: nominal
* text: nominal


## Distribution/histograms of values in the attributes
paste stuff from below


## Load raw sentiment140.csv

```{python}
dataset_path = 'datasets/sentiment/part-1-training.1600000.processed.noemoticon.csv.csv'
DATASET_COLUMNS = ["target", "ids", "date", "flag", "user", "text"]

df_raw = pd.read_csv(dataset_path, encoding ='latin_1' , names=DATASET_COLUMNS)
#df_raw.head()
```

### Preprocess: extract day from date to sep col

```{python}
def export_to_csv(df, file_name):
    print('write file to: ', file_name)
    df.to_csv(file_name, encoding='utf-8', index=False)

def clean_date(df):
    '''
    weekday extraction 1min
    '''
    def funcapply(x):
        return x[0:3]
    df['weekday'] = df['date'].apply(lambda x: funcapply(x))

    '''
    parse time to pandas datetime takes 5 minutes
    from datetime import datetime
    d = datetime.strptime('Thu Apr 23 13:38:19 +0000 2009','%a %b %d %H:%M:%S %z %Y').strftime('%Y-%m-%d %H:%M:%S');
    '''
    df['date'] = pd.to_datetime(df['date'])
    return df

def preprocess_tweet(tweet):
    #convert the tweet to lower case
    tweet = tweet.lower()
    #convert all urls to string "URL"
    tweet = re.sub('((www\.[^\s]+)|(https?://[^\s]+))','URL',tweet)
    #convert all @username to "AT_USER"
    #tweet = re.sub('@[^\s]+','AT_USER', tweet)
    #correct all multiple white spaces to a single white space
    tweet = re.sub('[\s]+', ' ', tweet)
    #convert "#topic" to just "topic"
    tweet = re.sub(r'#([^\s]+)', r'\1', tweet)
    return tweet

def preprocess(df):
    '''
    call all other functions
    create new columns
    '''
    print("clean date...")    
    # split and clean date
    df_new = clean_date(df)
    print("clean text...")
    # clean tweet text
    df_new['text'] = df_new['text'].apply(preprocess_tweet)
    print("add wc...")
    # add col text wordcount
    df_new['text_wc'] = [len(nltk.word_tokenize(t)) for t in df_new.text]
    print("add len...")
    # add col text string length
    df_new['text_len'] = [len(t) for t in df_new.text]
    return df_new
    
#df_clean = preprocess(df_raw)
#dataset_path = 'datasets/sentiment/sentiment140_date_clean.csv'
#export_to_csv(df_clean, './data/sentiment140_date_clean.csv')
```

## Read Clean sentiment140

```{python}
dataset_path = 'datasets/sentiment/sentiment140_date_clean.csv'
df_clean = pd.read_csv(dataset_path, encoding='utf-8', header=0)
df_clean['date'] = pd.to_datetime(df_clean['date'])
df_clean.head()
```

## Column Data Types 

```{python}
print(df_clean.dtypes)
```

## Shape and Size

```{python}
print("Dataset has ", df_clean.shape[0], "rows and ", df_clean.shape[1], "columns.")
```

## Sentiment Distribution

The plot below shows clearly that half of the datasets tweets are labeled positive and half negative.

```{python}
# @hidden_cell
target_cnt = df_clean.target.value_counts()
target_cnt = target_cnt.to_frame()
target_cnt.reset_index(inplace=True)
target_cnt.columns = ['SENTIMENT', 'COUNT']
target_cnt.SENTIMENT = target_cnt.SENTIMENT.map({4:'pos', 2:'neu', 0:'neg'})

plt.figure(figsize=(10,5))
plt.bar(target_cnt.SENTIMENT, target_cnt.COUNT)
plt.title("distribution of sentiment labels")
plt.xlabel("sentiment")
plt.ylabel("count")
plt.show()
```

## Distribution of tweets by days & Distribution of tweets by Weekdays

To show how the tweets are distributed according to their appearence in time we visualized tweets by the days of the month and of the week. Therefore we had to seperate the twitter timestamp into two different columns to be comfortable to process.

```{python}
g = df_clean.groupby(by=df_clean.date.dt.day).agg('count')

df_time_plot = pd.DataFrame({"day":g.index, "count":g.ids})

plt.rcParams["figure.figsize"] = (18,6)
ax = df_time_plot.plot.bar(x='day', y='count', rot=0)

plt.xlabel("monthday")
plt.ylabel("count")
plt.title("Distribution of tweets per Month days")
plt.show()
```

```{python}
g = df_clean.groupby(by=df_clean.weekday).agg('count')

df_weekday_plot = pd.DataFrame({"day":g.index, "count":g.ids})

cats = ['Mon','Tue','Wed','Thu','Fri','Sat', 'Sun']
df_weekday_plot.day = pd.Categorical(df_weekday_plot.day, 
                      categories=cats,
                      ordered=True)
df_weekday_plot.sort_values('day', inplace=True)
df_weekday_plot

plt.rcParams["figure.figsize"] = (18,6)
ax = df_weekday_plot.plot.bar(x='day', y='count', rot=0)

plt.xlabel("weekday")
plt.ylabel("count")
plt.title("Distribution of tweets per Week days")
plt.show()
```

## Distribution of text string length & text wordcount

Beside having the text as a feature vector we derived the wordcount as well as the tweet-string-length as another potential feature from the text column. It is also very handy to briefly introcude and visualize the text column. The two plots below show the how the **wordcount per tweet** and the **tweet-string-length** per tweet are distributed across the dataset. To further visualize the few outliers we introduced the box plot diagram at the right.

```{python}
plt.figure(1)
plt.figure(figsize=(18, 12))

plt.subplot(221)
plt.hist(df_clean.text_len, bins=400)
plt.title("distribution of tweet-string-length")
plt.xlabel("length")
plt.ylabel("count")

plt.subplot(222)
green_diamond = dict(markerfacecolor='b', marker='D')
plt.boxplot(df_clean.text_len, flierprops=green_diamond) 
plt.title("distribution of tweet-string-length [boxplot]")
plt.ylabel("count")

###

plt.subplot(223)
plt.hist(df_clean.text_wc, bins=300)
plt.xlabel("length")
plt.ylabel("count")
plt.title("distribution of words per tweet (wordcount)")

plt.subplot(224)
green_diamond = dict(markerfacecolor='b', marker='D')
plt.boxplot(df_clean.text_wc, flierprops=green_diamond) 
plt.title("distribution of words per tweet (wordcount) [boxplot]")
plt.ylabel("count")


fig.tight_layout()
plt.show()
```

# Data Set 2: OPPORTUNITY Activity Recognition Dataset

Abstract: "The OPPORTUNITY Dataset for Human Activity Recognition from Wearable, Object, and Ambient Sensors is a dataset devised to benchmark human activity recognition algorithms (classification, automatic data segmentation, sensor fusion, feature extraction, etc). <sup>1</sup>

<sup>1</sup> https://archive.ics.uci.edu/ml/datasets/OPPORTUNITY+Activity+Recognition#


## Explanation

We chose this dataset since for its high dimensionality and because it's interesting for us to work with sensor data from wearable devices. Additionally, it is very different from the first data set less samples per user per run while having many features. 

## Characteristics

### How many samples:

Notes regarding the recordings of the dataset:
* 4 users, 
* 6 runs per users. 
* Of these, 5 are **Activity of Daily Living (644.635 samples)** runs characterized by a natural execution of daily activities, where samples per user break down the following way:
    * 1    179.695 samples
    * 2    171.785 samples
    * 4    147.214 samples
    * 3    145.941 samples
* The 6th run is a **"drill" (118.975 samples)** run, where users execute a scripted sequence of activities. Samples per user break down the following way:
    * 2    318.26 samples
    * 4    305.27 samples
    * 1    301.27 samples
    * 3    264.95 samples

### How many attributes (features):

The dataset comprises the readings of motion sensors recorded while users executed typical daily activities.The attributes correspond to raw sensor readings. There is a total of 242 attributes (all of type float). 

* **Body-worn sensors (145 attributes):** The body-worn sensors include 7 inertial measurement units and 12 3D acceleration sensors.The inertial measurement units provide readings of: 3D acceleration, 3D rate of turn, 3D magnetic field, and orientation of the sensor with respect to a world coordinate system in quaternions. Five sensors are on the upper body and two are mounted on the user's shoes. The acceleration sensors provide 3D acceleration. They are mounted on the upper body, hip and leg. Four tags for an ultra-wideband localization system are placed on the left/right front/back side of the shoulder. 

* **Object sensors (60 attributes):** 12 objects are instrumented with wireless sensors measuring 3D acceleration and 2D rate of turn. This allows to detect which objects are used, and possibly also the kind of usage that is made of them. 

* **Ambient sensors (37 attributes):** Ambient sensors include 13 switches and 8 3D acceleration sensors in drawers, kitchen appliances and doors. The reed switches are placed in triplets on the fridge, dishwasher and drawer 2 and drawer 3. They may be used to detect three states of the furniture element: closed, half open, and fully open. The acceleration sensors may allow to assess if an element of furniture is used, and whether it may be opened or closed.

### What types of attributes (nominal, ordinal, interval, ...):


Columns:
* 1 - duration - milli seconds **(ratio)** 
* 2-37 - Accelerometers with the unit: milli g **(interval)** 
* 38-134 - InertialMeasurementUnit withe the unit millig g and mm/s **(interval and ratio)**
* 135-194 - Accelerometers - milli g (interval)
* 195 - 207 - Logical binary values - on/off 0/1 **(nominal)**
* 208-231 - Accelerometer - milli g **(interval)**
* 232 - 242 - LOCATION - millimeters **(ratio)**
* 243-250 - Legend one-hot-encoded locomotion information - yes/no **(nominal)**


```{python colab={'base_uri': 'https://localhost:8080/', 'height': 8704}, colab_type=code, id=FUoaE5eIXrTd, outputId=45a93522-3153-4e11-de37-e2d5f7a99ea8}
# build the data dictionary
opportunity_cols = dict()
opportunity_cols_txt = open("datasets/opportunity/column_names.txt", "r")

for line in opportunity_cols_txt:

    if re.search('Column',line) != None:
        col_id = re.search('Column: (\S*) ',line).group(1)
        opportunity_cols[col_id] = dict()
        opportunity_cols[col_id]['name'] = re.search('Column: \S* (\S*)',line).group(1)

        if re.search('Column: \S* \S* \S*',line) != None:
            opportunity_cols[col_id]['sensor'] = re.search('Column: \S* \S* (\S*)',line).group(1)
            opportunity_cols[col_id]['sensor_axis'] = re.search(' (\S*);',line).group(1)
            opportunity_cols[col_id]['value_type'] = re.search('value = (.*),',line).group(1)
            opportunity_cols[col_id]['unit'] = re.search('unit =(.*)$',line).group(1)     
#opportunity_cols

# and generate the column names for the data frame
col_names = list()
for key, column in opportunity_cols.items():
    col_names.append(key+"_"+column.get('name'))
col_names.insert(0,'0_User')
#col_names
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 71}, colab_type=code, id=oqO1Zom-S0Qs, outputId=71f8505a-ca31-4b7a-fb03-1c2880a8a280}
adl_filename_mask = 'S{}-ADL{}.dat'
drill_filename_mask = 'S{}-Drill.dat'
df_opportunity_adl = pd.DataFrame()
df_opportunity_drill = pd.DataFrame()

# go through the data fractions for the 4 users
for user_idx in range(1,5):
    # go through 5 adl runs and read in fractions
    for run in range (1,6):
        path = 'datasets/opportunity/'+adl_filename_mask.format(user_idx, run)
        df_partial_adl = pd.read_csv(path, header=None, sep='\s')
        df_partial_adl.insert(0,'User',user_idx)
        df_opportunity_adl = df_opportunity_adl.append(df_partial_adl) 
    
    # also add the 6th "drill" run to the dataframe
    path = 'datasets/opportunity/'+adl_filename_mask.format(user_idx, run)
    df_partial_drill = pd.read_csv(path, header=None, sep='\s')
    df_partial_drill.insert(0,'User',user_idx)
    df_opportunity_drill = df_opportunity_drill.append(df_partial_adl)

print(df_opportunity_adl.shape)

#df_adl = pd.read_csv('drive/My Drive/University/Data Science/Machine Learning/datasets/opportunity/S1-ADL1.dat', encoding='latin_1', header=None, sep='\s')
```

```{python}
# in order to not always do the read in of the fractions 

#export_to_csv(df_opportunity_adl, "datasets/opportunity/opportunity_adl.csv")
#export_to_csv(df_opportunity_drill, "datasets/opportunity/oppportunity_drill.csv")
```

```{python}
# already read in from the prejoined fractions

df_opportunity_adl = pd.read_csv('datasets/opportunity/opportunity_adl.csv')
df_opportunity_drill = pd.read_csv('datasets/opportunity/opportunity_drill.csv')

print(df_opportunity_adl.shape)
print(df_opportunity_drill.shape)
```

```{python}
# set column names according to data dictionary 
df_opportunity_adl.columns = col_names
df_opportunity_drill.columns = col_names
```

```{python}
# get sample sizes for the two types of runs per user
print("ADL:")
print(df_opportunity_adl['0_User'].value_counts())
print("\nDrill")
print(df_opportunity_drill['0_User'].value_counts())
```

```{python}
df_opportunity_adl.describe()
```

```{python}
df_opportunity_adl.dtypes.value_counts()
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 388}, colab_type=code, id=KIXW9NALTHHf, outputId=5bee476f-9eb9-4c0d-a859-97a1e8e75814}
df_opportunity_adl.head(10)
```
