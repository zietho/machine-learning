{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set 1: sentiment140\n",
    "\n",
    "Abstract: This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment. <sup>1</sup>\n",
    "\n",
    "<sup>1</sup> https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw sentiment140.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'datasets/sentiment/part-1-training.1600000.processed.noemoticon.csv.csv'\n",
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "df_raw = pd.read_csv(dataset_path, encoding ='latin_1' , names=DATASET_COLUMNS)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess: extract day from date to sep col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(df, file_name):\n",
    "    print('write file to: ', file_name)\n",
    "    df.to_csv(file_name, encoding='utf-8', index=False)\n",
    "\n",
    "def clean_date(df):\n",
    "    '''\n",
    "    weekday extraction 1min\n",
    "    '''\n",
    "    def funcapply(x):\n",
    "        return x[0:3]\n",
    "    df['weekday'] = df['date'].apply(lambda x: funcapply(x))\n",
    "\n",
    "    '''\n",
    "    parse time to pandas datetime takes 5 minutes\n",
    "    from datetime import datetime\n",
    "    d = datetime.strptime('Thu Apr 23 13:38:19 +0000 2009','%a %b %d %H:%M:%S %z %Y').strftime('%Y-%m-%d %H:%M:%S');\n",
    "    '''\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    #convert the tweet to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #convert all urls to string \"URL\"\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #convert all @username to \"AT_USER\"\n",
    "    #tweet = re.sub('@[^\\s]+','AT_USER', tweet)\n",
    "    #correct all multiple white spaces to a single white space\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #convert \"#topic\" to just \"topic\"\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess(df):\n",
    "    '''\n",
    "    call all other functions\n",
    "    create new columns\n",
    "    '''\n",
    "    print(\"clean date...\")    \n",
    "    # split and clean date\n",
    "    df_new = clean_date(df)\n",
    "    print(\"clean text...\")\n",
    "    # clean tweet text\n",
    "    df_new['text'] = df_new['text'].apply(preprocess_tweet)\n",
    "    print(\"add wc...\")\n",
    "    # add col text wordcount\n",
    "    df_new['text_wc'] = [len(nltk.word_tokenize(t)) for t in df_new.text]\n",
    "    print(\"add len...\")\n",
    "    # add col text string length\n",
    "    df_new['text_len'] = [len(t) for t in df_new.text]\n",
    "    return df_new\n",
    "    \n",
    "#df_clean = preprocess(df_raw)\n",
    "#dataset_path = 'datasets/sentiment/sentiment140_date_clean.csv'\n",
    "#export_to_csv(df_clean, './data/sentiment140_date_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Clean sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'datasets/sentiment/sentiment140_date_clean.csv'\n",
    "df_clean = pd.read_csv(dataset_path, encoding='utf-8', header=0)\n",
    "df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Data Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape and Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset has \", df_clean.shape[0], \"rows and \", df_clean.shape[1], \"columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cnt = df_clean.target.value_counts()\n",
    "target_cnt = target_cnt.to_frame()\n",
    "target_cnt.reset_index(inplace=True)\n",
    "target_cnt.columns = ['SENTIMENT', 'COUNT']\n",
    "target_cnt.SENTIMENT = target_cnt.SENTIMENT.map({4:'pos', 2:'neu', 0:'neg'})\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(target_cnt.SENTIMENT, target_cnt.COUNT)\n",
    "plt.title(\"Sentiment label distribuition\")\n",
    "\n",
    "target_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution by days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df_clean.groupby(by=df_clean.date.dt.day).agg('count')\n",
    "\n",
    "df_time_plot = pd.DataFrame({\"day\":g.index, \"count\":g.ids})\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "ax = df_time_plot.plot.bar(x='day', y='count', rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution by Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df_clean.groupby(by=df_clean.weekday).agg('count')\n",
    "\n",
    "df_weekday_plot = pd.DataFrame({\"day\":g.index, \"count\":g.ids})\n",
    "# sort categorical\n",
    "cats = ['Mon','Tue','Wed','Thu','Fri','Sat', 'Sun']\n",
    "df_weekday_plot.day = pd.Categorical(df_weekday_plot.day, \n",
    "                      categories=cats,\n",
    "                      ordered=True)\n",
    "df_weekday_plot.sort_values('day', inplace=True)\n",
    "df_weekday_plot\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,8)\n",
    "ax = df_weekday_plot.plot.bar(x='day', y='count', rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distribution of text string length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.hist(df_clean.text_len, bins=400)\n",
    "plt.title(\"distribution of string length\")\n",
    "plt.xlabel(\"length\")\n",
    "plt.xlabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "green_diamond = dict(markerfacecolor='b', marker='D')\n",
    "plt.boxplot(df_clean.text_len, flierprops=green_diamond) # plot pre_clean_len column\n",
    "plt.title(\"distribution of string length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distribution of text wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.hist(df_clean.text_wc, bins=300)\n",
    "plt.xlabel(\"length\")\n",
    "plt.xlabel(\"count\")\n",
    "plt.title(\"wordcount of string length\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "green_diamond = dict(markerfacecolor='b', marker='D')\n",
    "plt.boxplot(df_clean.text_wc, flierprops=green_diamond) # plot pre_clean_len column\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set 1: OPPORTUNITY Activity Recognition Dataset\n",
    "\n",
    "Abstract: \"The OPPORTUNITY Dataset for Human Activity Recognition from Wearable, Object, and Ambient Sensors is a dataset devised to benchmark human activity recognition algorithms (classification, automatic data segmentation, sensor fusion, feature extraction, etc). <sup>1</sup>\n",
    "\n",
    "<sup>1</sup> https://archive.ics.uci.edu/ml/datasets/OPPORTUNITY+Activity+Recognition#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# activity of daily living (ADL)\n",
    "Read in activity of daily living (ADL) for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 8704
    },
    "colab_type": "code",
    "id": "FUoaE5eIXrTd",
    "outputId": "45a93522-3153-4e11-de37-e2d5f7a99ea8"
   },
   "outputs": [],
   "source": [
    "# read in column names\n",
    "opportunity_cols = dict()\n",
    "opportunity_cols_txt = open(\"datasets/opportunity/column_names.txt\", \"r\")\n",
    "\n",
    "for line in opportunity_cols_txt:\n",
    "\n",
    "    if re.search('Column',line) != None:\n",
    "        col_id = re.search('Column: (\\S*) ',line).group(1)\n",
    "        opportunity_cols[col_id] = dict()\n",
    "        opportunity_cols[col_id]['name'] = re.search('Column: \\S* (\\S*)',line).group(1)\n",
    "\n",
    "        if re.search('Column: \\S* \\S* \\S*',line) != None:\n",
    "            opportunity_cols[col_id]['sensor'] = re.search('Column: \\S* \\S* (\\S*)',line).group(1)\n",
    "            opportunity_cols[col_id]['sensor_axis'] = re.search(' (\\S*);',line).group(1)\n",
    "            opportunity_cols[col_id]['value_type'] = re.search('value = (.*),',line).group(1)\n",
    "            opportunity_cols[col_id]['unit'] = re.search('unit =(.*)$',line).group(1)\n",
    "            \n",
    "#opportunity_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = list()\n",
    "for key, column in opportunity_cols.items():\n",
    "    col_names.append(key+\"_\"+column.get('name'))\n",
    "col_names.insert(0,'0_User')\n",
    "#col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "oqO1Zom-S0Qs",
    "outputId": "71f8505a-ca31-4b7a-fb03-1c2880a8a280"
   },
   "outputs": [],
   "source": [
    "adl_filename_mask = 'S{}-ADL{}.dat'\n",
    "drill_filename_mask = 'S{}-Drill.dat'\n",
    "df_opportunity_adl = pd.DataFrame()\n",
    "df_opportunity_drill = pd.DataFrame()\n",
    "\n",
    "for user_idx in range(1,5):\n",
    "    for run in range (1,6):\n",
    "        path = 'datasets/opportunity/'+adl_filename_mask.format(user_idx, run)\n",
    "        df_partial_adl = pd.read_csv(path, header=None, sep='\\s')\n",
    "        df_partial_adl.insert(0,'User',user_idx)\n",
    "        df_opportunity_adl = df_opportunity_adl.append(df_partial_adl) \n",
    "    \n",
    "    path = 'datasets/opportunity/'+adl_filename_mask.format(user_idx, run)\n",
    "    df_partial_drill = pd.read_csv(path, header=None, sep='\\s')\n",
    "    df_partial_drill.insert(0,'User',user_idx)\n",
    "    df_opportunity_drill = df_opportunity_drill.append(df_partial_adl)\n",
    "\n",
    "print(df_opportunity_adl.shape)\n",
    "\n",
    "#df_adl = pd.read_csv('drive/My Drive/University/Data Science/Machine Learning/datasets/opportunity/S1-ADL1.dat', encoding='latin_1', header=None, sep='\\s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export_to_csv(df_opportunity_adl, \"datasets/opportunity/opportunity_adl.csv\")\n",
    "#export_to_csv(df_opportunity_drill, \"datasets/opportunity/oppportunity_drill.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opportunity_adl = pd.read_csv('datasets/opportunity/opportunity_adl.csv')\n",
    "df_opportunity_drill = pd.read_csv('datasets/opportunity/opportunity_drill.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opportunity_adl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opportunity_adl.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoiEAWOMXp3E"
   },
   "outputs": [],
   "source": [
    "df_opportunity_adl.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "id": "KIXW9NALTHHf",
    "outputId": "5bee476f-9eb9-4c0d-a859-97a1e8e75814"
   },
   "outputs": [],
   "source": [
    "df_opportunity_adl.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ich bin ein hübsches Markdown"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
